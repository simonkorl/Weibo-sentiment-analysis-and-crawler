# 微博情感分析与爬虫
微博是什么就不多说了。 
进行情感分析的好处是可以将用户分类为各种类别，并将相应的广告推送给他们，或者打开你的脑洞来一场头脑风暴。在这个任务里，我选择进行的情绪分析是将用户分为真实用户和水军/营销粉。 根据有关微博水军检测的大多数论文，他们使用的方法主要是通过对用户的各种指标（关注度，粉丝数，平均发微博时间等）进行逻辑回归从而对用户进行分类。 我认为这种方法的准确性不高，并且对于不同的测试集会表现得不稳定。我认为此类任务需要NLP模型的帮助，因为水军和真人之间的最大区别是他们编写微博的行为习惯。
### 重要的事情说三遍：
*请用google colab，由于要翻墙，请百度“不翻墙,使用colab”*  
**请用google colab，由于要翻墙，请百度“不翻墙,使用colab”**  
***请用google colab，由于要翻墙，请百度“不翻墙,使用colab”***  
由于防止炼丹走火入魔，本库白嫖了谷歌colab的免费TPU，请务必使用colab。在这抛两个notebook，[这是水军测试程序](https://colab.research.google.com/drive/1ziFANxlCILg2nCzKCNblqqr1w2Ds0kie#scrollTo=m-8fCAt3SAX-) and [这是训练和测试的过程](https://colab.research.google.com/drive/1GsTUjyWvHWKiHURCO7vFldWaEt6lUDRj)

## 模型输入输出的结构
```
输入
│── 用户信息指标:[关注数, 粉丝数, 互动数, 会员等级, 会员类别, 发微博总数, 微博等级, 是否认证, 认证类型]
│── 最新的第1条微博
│   │──  正文
│   │──  话题/超话
│   │──  被转发微博的特征:[话题/超话,有否视频及其关键信息?,有否照片及其关键信息?] 或者 "无转发"
│   └──  博文信息指标:[贴图数量,视频播放量,转发数,评论数,点赞数]
│── 最新的第2条微博
│── 最新的第...条微博
│── 最新的第n-1条微博
└── 最新的第n条微博
输出
│── 是真实用户
└── 是水军
```
## 模型的架构
本人认为，仅通过一条随机微博来分析用户是不够的。我们需要分析来自单个用户的连续微博。也就是说，对每n条微博并行执行情感分析（我使用Bi-LSTM模型），然后将这n个输出（尝试将它们视为一个句子的tokens）放入网络中，最后得到分类。此外，人类有书写的习惯，例如：有些人每三天都发布快乐的内容，然后下一天会发布严肃的内容，有些人则可能每天只发布悲伤的内容。假设小明（小明：“怎么又是我？找小刚吧”）最近的8条微博将是 ***[开心，开心，开心，严肃，开心，开心，开心，严肃]*** ，所以我们知道他在每8条微博中就会有2条严肃的微博。尽管 ***[严肃，严肃，开心，开心，开心，开心，开心，开心，开心]*** 具有相同的发微博类型的频率，但是由于顺序不同，这个序列的“形状”就不同，我们不能说这就是小明的习惯。然后 ***[开心，开心，严肃，开心，开心，开心，严肃，开心]*** 只是序列左移了一个单位（多少单位也没关系），但它的“形状”是一样的，也可以说这是小明的习惯。因此，用于连接n个输出的网络也将成为递归模型（我再次使用LSTM）。由于这是一个嵌套的并行LSTM模型，为了防止梯度消失，我使用的大多数激活函数是Tanh，并且在某些层上执行了40％Dropout以防止过拟合。下面是模型的结构。  
![image](https://github.com/timmmGZ/Weibo-sentiment-analysis-and-crawler/blob/main/images/weibo.png?raw=true)  
## 爬虫
我有一个包含568个样本的*user_id*数据集（274个水军和294个真实用户）。 所有样本都是通过人工检查来进行标注，以尽量确保数据集的逻辑性和分布情况，从而客观地保障了测试集准确性的公平性。
将*user_id*数据集输入到我的爬虫代码中，然后它会为模型输出新的数据集（如上所述，“模型输入的结构”）。 同时，{正文} 和 {话题/超话；被转发微博的特征} 具有非常不同的语法，词汇量和句子长度。 因此，在做embedding时，我为它们分别创建了各自的词典，这也可以使得 {话题/超话；被转发微博的特征} 在作为one-hot编码输入embedding前得到降维效果，可谓一举两得。
## 一些baselines的结果
  
<table>
  <tr><th>训练集的拆分率</th><th>测试集准确率</th><th>n条微博</th><th>baseline文件</th></tr>
  <tr><td>85% </td>
    <td>98.84%</td><td>20</td><td rowspan="3"><a href="https://github.com/timmmGZ/Weibo-sentiment-analysis-and-crawler/tree/main/weibo_baselines">weibo_baselines</a></td></tr>
  <tr><td>50% </td><td>90.14%</td><td>20</td></tr>
  <tr><td>15% </td><td>90.48%</td><td>20</td></tr>
</table>

“20条微博”据集字典具有27890个tokens。 每个不同的训练集都有不同的词典。 例如，当训练集为数据集的85％时，词典中有25000个tokens。 当它是15％时，有10000个tokens。 但是，即使测试集中有太多未知tokens（词汇），所有测试集的准确率仍高于90％。
## 题外话
有意者可以标注更多的数据集，格式如下：
  uid  | 是否水军
------------- | -------------
  532871947（乱写的） | 0
  214839591（乱写的） | 1  
  
 又或者是其他情感分析
   uid  | 音乐|美术|舞蹈|...
------------- | -------------| -------------| -------------| -------------
  532871947（乱写的） | 1|1|0|...
  214839591（乱写的） | 0|1|1|...
  
  
uid  | 喜欢音乐的程度
------------- | -------------
  532871947（乱写的） | 5
  214839591（乱写的） | 0
