{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Weibo_sentiment_analysis_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8GsYdzbTpmk"
      },
      "source": [
        "#Download\r\n",
        "Download 3 things that saved during weibo.ipynb:\r\n",
        "* Dictionaries for One-hot encoding in Embeddinng layers.  \r\n",
        "* metrics' means and standard deviations (for input normalization). Because the data is normalized by $(data-mean)/std$ before training the model, the same means and stds have to be used to normalize the new dataset.  \r\n",
        "* model and its weights\r\n",
        "\r\n",
        "P.S. For those who understand Chinese language or use weibo, you can create a csv file with one column named \"uid\", and then fill in the user ids to be detected. Please save it as \"uids_for_detection.csv\" and upload it.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-8fCAt3SAX-",
        "outputId": "b8fb5508-4cd4-4e7b-d76e-231769db11b4"
      },
      "source": [
        "import pandas as pd\r\n",
        "!gdown https://drive.google.com/uc?id=1nbWJO9wGMODfnmYGCASi9kEozpgtIRjo\r\n",
        "!unzip weibo_baselines.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nbWJO9wGMODfnmYGCASi9kEozpgtIRjo\n",
            "To: /content/weibo_baselines.zip\n",
            "38.9MB [00:00, 106MB/s] \n",
            "Archive:  weibo_baselines.zip\n",
            "   creating: weibo0.15split_testAcc90.48%/\n",
            "   creating: weibo0.50split_testAcc90.14%/\n",
            "   creating: weibo0.85split_testAcc98.84%/\n",
            "  inflating: uids_for_detection.csv  \n",
            "  inflating: weibo0.50split_testAcc90.14%/metrics_means_and_stds.csv  \n",
            "  inflating: weibo0.50split_testAcc90.14%/weibo_model.h5  \n",
            "  inflating: weibo0.50split_testAcc90.14%/token2idW.csv  \n",
            "  inflating: weibo0.50split_testAcc90.14%/token2idTR.csv  \n",
            "  inflating: weibo0.50split_testAcc90.14%/weibo_dataset.csv  \n",
            "   creating: weibo0.85split_testAcc98.84%/.ipynb_checkpoints/\n",
            "  inflating: weibo0.85split_testAcc98.84%/metrics_means_and_stds.csv  \n",
            "  inflating: weibo0.85split_testAcc98.84%/weibo_model.h5  \n",
            "  inflating: weibo0.85split_testAcc98.84%/token2idW.csv  \n",
            "  inflating: weibo0.85split_testAcc98.84%/token2idTR.csv  \n",
            "  inflating: weibo0.85split_testAcc98.84%/weibo_dataset.csv  \n",
            "   creating: weibo0.15split_testAcc90.48%/.ipynb_checkpoints/\n",
            "  inflating: weibo0.15split_testAcc90.48%/metrics_means_and_stds.csv  \n",
            "  inflating: weibo0.15split_testAcc90.48%/weibo_model.h5  \n",
            "  inflating: weibo0.15split_testAcc90.48%/token2idW.csv  \n",
            "  inflating: weibo0.15split_testAcc90.48%/token2idTR.csv  \n",
            "  inflating: weibo0.15split_testAcc90.48%/weibo_dataset.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8iq9UqriGjG"
      },
      "source": [
        "file_name='uids_for_detection.csv'\r\n",
        "uid_label_dataset=pd.read_csv(file_name,sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR0VLnXrfAkE"
      },
      "source": [
        "#Crawler, crawl the weibo information corresponding to uids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXR6cw3kSOXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8423e3-33ad-416c-9404-d499db3ff21a"
      },
      "source": [
        "%pylab inline\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from IPython.display import clear_output\r\n",
        "dataset_csv=pd.DataFrame()\r\n",
        "index=0\r\n",
        "while index< len(uid_label_dataset):\r\n",
        "  row=uid_label_dataset.iloc[index]\r\n",
        "  print(\"Crawling the %s th user\"%(index+1))\r\n",
        "  try:\r\n",
        "    userid=str(row[0])\r\n",
        "    headers = {'User_Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\r\n",
        "    def get_json(param):\r\n",
        "      return requests.get('https://m.weibo.cn/api/container/getIndex?containerid='+param,headers=headers,verify=False).json()\r\n",
        "    js = get_json('100505'+userid)\r\n",
        "    info_keys=['follow_count', 'followers_count', 'mbrank', 'mbtype', 'statuses_count', 'urank', 'verified', 'verified_type', 'verified_type_ext']\r\n",
        "    if js['ok']:\r\n",
        "      info={k:v for k,v in js['data']['userInfo'].items() if k in info_keys }\r\n",
        "      if not info.get('verified_type_ext'):info['verified_type_ext']=-1\r\n",
        "      fans_interact=get_json('231051_-_fans_intimacy_-_'+userid)['data']['cards']\r\n",
        "      info['interact']=-1\r\n",
        "      if js['ok']:\r\n",
        "        if any([\"没有\" in f.get('desc',\"\") for f in fans_interact]):\r\n",
        "          info['interact']=0\r\n",
        "        else:\r\n",
        "          info['interact']=sum([int(f.get('desc1').split()[1][3:]) for f in fans_interact[1]['card_group']])\r\n",
        "      page=1\r\n",
        "      js=get_json('107603' + userid+'&count='+str(30)+'&page='+str(page))\r\n",
        "      if js['ok']:\r\n",
        "        weibo_infos,topics,retwefeatuers,weibos,num_pics,video_play_counts,reposts_counts,comments_counts,attitudes_counts=[],[],[],[],[],[],[],[],[]\r\n",
        "        for i in range(len(js['data']['cards'])):\r\n",
        "          weibos.append('\\n'.join([l for l in list(BeautifulSoup(js['data']['cards'][i]['mblog']['text'],'lxml').stripped_strings) if \"@\" not in l and \"#\" not in l]))\r\n",
        "          num_pics.append(len(js['data']['cards'][i]['mblog'].get('pic_ids',[])))\r\n",
        "          video_play_counts.append(int(js['data']['cards'][i]['mblog'].get('page_info',{\"play_count\":\"-1次播放\"}).get('play_count',\"-1次播放\")[:-3].replace(\"万\",\"0000\").replace(\"亿\",\"00000000\")))\r\n",
        "          reposts_counts.append(int(str(js['data']['cards'][i]['mblog']['reposts_count']).replace(\"万\",\"0000\").replace(\"亿\",\"00000000\").replace(\"+\",\"\")))\r\n",
        "          comments_counts.append(int(str(js['data']['cards'][i]['mblog']['comments_count']).replace(\"万\",\"0000\").replace(\"亿\",\"00000000\").replace(\"+\",\"\")))\r\n",
        "          attitudes_counts.append(int(str(js['data']['cards'][i]['mblog']['attitudes_count']).replace(\"万\",\"0000\").replace(\"亿\",\"00000000\").replace(\"+\",\"\")))\r\n",
        "          weibo_infos.append([num_pics[i],video_play_counts[i],reposts_counts[i],comments_counts[i],attitudes_counts[i]])\r\n",
        "          topics.append('\\n'.join([l for l in list(BeautifulSoup(js['data']['cards'][i]['mblog']['text'],'lxml').stripped_strings) if  \"#\"  in l]))\r\n",
        "          retwefeatuers.append('\\n'.join([l for l in list(BeautifulSoup(js['data']['cards'][i]['mblog'].get('retweeted_status',{\"text\":\"noretweet\"}).get(\"text\"),'lxml').stripped_strings) if \"#\"in l or \"视频\" in l or \"noretweet\" in l]))\r\n",
        "        info['n_weibos']=str(weibos)\r\n",
        "        info['n_topics']=str(topics)\r\n",
        "        info['n_weibo_infos']=str(weibo_infos)\r\n",
        "        info['n_retwefeatuers']=str(retwefeatuers)\r\n",
        "        info['uid']=int(row[0])\r\n",
        "        dataset_csv=dataset_csv.append(info, ignore_index=True)\r\n",
        "    index+=1\r\n",
        "    if index%20==0:\r\n",
        "      dataset_csv.fillna(-1,inplace=True)\r\n",
        "      dataset_csv.to_csv('weibo_dataset.csv', index=False,encoding=\"utf_8_sig\")\r\n",
        "    clear_output()\r\n",
        "  except Exception as e:\r\n",
        "    print(e)\r\n",
        "    time.sleep(5+np.random.rand()*3)\r\n",
        "    pass\r\n",
        "print(\"The crawling of \"+str(index)+\" users has ended (except for some users who have been blocked) and has been saved to \\\"weibo_dataset.csv\\\"\")\r\n",
        "dataset_csv.fillna(-1,inplace=True)\r\n",
        "dataset_csv.to_csv('weibo_dataset.csv', index=False,encoding=\"utf_8_sig\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The crawling of 2 users has ended (except for some users who have been blocked) and has been saved to \"weibo_dataset.csv\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmG8WmSPfLPx"
      },
      "source": [
        "#Load framework and TPU environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eQGlkroe3OX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc80742-038b-4c9e-d9ac-f4e53d34434e"
      },
      "source": [
        "%pylab inline\r\n",
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import jieba\r\n",
        "from gensim import corpora\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from scipy import stats\r\n",
        "import ast\r\n",
        "import os\r\n",
        "import keras\r\n",
        "from keras.models import Model,Sequential\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.layers import Dense, Reshape,Input, LSTM,Bidirectional,GRU, concatenate\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from keras.callbacks import ReduceLROnPlateau\r\n",
        "from keras import optimizers\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "\r\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n",
        "tf.config.experimental_connect_to_cluster(resolver)\r\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\r\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['info']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.32.137.130:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.32.137.130:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQGyc6eLfZFp"
      },
      "source": [
        "#Load the model and its weights\r\n",
        "I choose the one trained with 85% data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgo_9zF8ilS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96330c40-0edc-48a2-80c1-e2088a15e6f3"
      },
      "source": [
        "cd weibo0.85split_testAcc98.84%/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/weibo0.85split_testAcc98.84%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecHn16SiScdE"
      },
      "source": [
        "with strategy.scope():\r\n",
        "  model = tf.keras.models.load_model(\"weibo_model.h5\")\r\n",
        "maxlens=[i[1] for i in model.input_shape[0]][:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzi4i2D2fbpC"
      },
      "source": [
        "#Load metrics' means and standard deviations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R72jFfW1cxsE"
      },
      "source": [
        "metrics_means_and_stds=pd.read_csv('metrics_means_and_stds.csv')\r\n",
        "wmd_mean=ast.literal_eval(metrics_means_and_stds['weibo_metrics_dataset_mean'][0])\r\n",
        "wmd_std=ast.literal_eval(metrics_means_and_stds['weibo_metrics_dataset_std'][0])\r\n",
        "umd_mean=ast.literal_eval(metrics_means_and_stds['user_metrics_dataset_mean'][0])\r\n",
        "umd_std=ast.literal_eval(metrics_means_and_stds['user_metrics_dataset_std'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V6VB6YCfv06"
      },
      "source": [
        "#Read the test set from csv file and convert it to the input format of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDfNq4ao-jlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef6d2b79-568c-46d6-de12-1447119779ee"
      },
      "source": [
        "maxlens=[i[1] for i in model.input_shape[0]]\r\n",
        "num_weibo=len(model.input_shape)-1\r\n",
        "testset_dataset=pd.read_csv('../weibo_dataset.csv',keep_default_na=False)\r\n",
        "dicW=pd.read_csv('token2idW.csv',keep_default_na=False)\r\n",
        "dicW={k:v for k,v in dicW.values}\r\n",
        "dicTR=pd.read_csv('token2idTR.csv',keep_default_na=False)\r\n",
        "dicTR={k:v for k,v in dicTR.values}\r\n",
        "n_weibos=testset_dataset['n_weibos'].values\r\n",
        "def chinese_cut_token(sentences,dic=None):\r\n",
        "  sentences_cut=[]\r\n",
        "  for input in sentences:\r\n",
        "    input=ast.literal_eval(input)\r\n",
        "    sentence_cut=[[\"None\"]]*num_weibo\r\n",
        "    for i,weibo in enumerate(input):\r\n",
        "      if (i>=num_weibo): break\r\n",
        "      tokens=jieba.cut(weibo, cut_all=True)\r\n",
        "      sentence_cut[i]=[word for word in tokens]\r\n",
        "    sentences_cut.append(sentence_cut[:num_weibo])\r\n",
        "  dic=corpora.Dictionary([token for sentence_cut in sentences_cut for token in sentence_cut]).token2id if dic==None else dic\r\n",
        "  X=[]\r\n",
        "  for user in sentences_cut:\r\n",
        "    x=[]\r\n",
        "    for weibo_sentence in user:\r\n",
        "      x.append([dic.get(token,-1)+2 for token in weibo_sentence])\r\n",
        "    X.append(x)\r\n",
        "  maxLen=max(len(max(x,key=len)) for x in X)\r\n",
        "  wordsSize =len(dic)+1\r\n",
        "  return sentences_cut,dic,array(X),maxLen,wordsSize\r\n",
        "_,_,XW,_,_=chinese_cut_token(n_weibos,dicW)\r\n",
        "maxLenW=maxlens[0]\r\n",
        "n_weibos_seq2 = sequence.pad_sequences(XW.reshape(-1), maxLenW).reshape(len(n_weibos),num_weibo,maxLenW)\r\n",
        "n_topicsAndRetwefeatuers2=append(testset_dataset['n_retwefeatuers'],testset_dataset['n_topics'])\r\n",
        "_,_,XTR,_,_=chinese_cut_token(n_topicsAndRetwefeatuers2,dicTR)\r\n",
        "maxLenTR=maxlens[1]\r\n",
        "n_topicsAndRetwefeatuers_seq2 = sequence.pad_sequences(XTR.reshape(-1), maxLenTR).reshape(len(n_topicsAndRetwefeatuers2),num_weibo,maxLenTR)\r\n",
        "n_retwefeatuers_seq2=n_topicsAndRetwefeatuers_seq2[:len(testset_dataset['n_retwefeatuers'])]\r\n",
        "n_topics_seq2=n_topicsAndRetwefeatuers_seq2[len(testset_dataset['n_retwefeatuers']):]\r\n",
        "user_metrics_dataset2=testset_dataset[['follow_count', 'followers_count', 'mbrank', 'mbtype', 'statuses_count', 'urank', 'verified', 'verified_type', 'verified_type_ext']]\r\n",
        "user_metrics_dataset2=(user_metrics_dataset2-umd_mean)/umd_std\r\n",
        "tmp=[ast.literal_eval(row) for row in testset_dataset['n_weibo_infos']]\r\n",
        "weibo_metrics_dataset2=ones((len(tmp),num_weibo,len(tmp[0][0])))*-2\r\n",
        "for i in range(weibo_metrics_dataset2.shape[0]):\r\n",
        "  weibo_metrics_dataset2[i,:len(tmp[i])]=tmp[i][:num_weibo]\r\n",
        "weibo_metrics_dataset2=array([(wi-wmd_mean)/wmd_std for wi in weibo_metrics_dataset2])\r\n",
        "testInputs=[*[(n_weibos_seq2[:,i],n_retwefeatuers_seq2[:,i],n_topics_seq2[:,i],weibo_metrics_dataset2[:,i])for i in range(num_weibo)],user_metrics_dataset2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.967 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.967 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0XOMhgEpDMf"
      },
      "source": [
        "#Whether you know Chinese or not, please check these two links, then check the predicted result.\r\n",
        "https://m.weibo.cn/u/2214257545  \r\n",
        "https://m.weibo.cn/u/1847677113  \r\n",
        "Result will be saved in \"weibo_detection_result.csv\" \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZztDaPHepYCB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "c72d61c9-3f9e-4e15-d698-3dab72a7bfd2"
      },
      "source": [
        "pred=model.predict(testInputs).argmax(axis=1)\r\n",
        "output_csv=pd.DataFrame(columns=['uid','bot or not'])\r\n",
        "output_csv['uid']=testset_dataset['uid'].astype(int)\r\n",
        "output_csv['bot or not']=pred.astype(bool)\r\n",
        "output_csv.to_csv('../weibo_detection_result.csv',index=False,encoding=\"utf_8_sig\")\r\n",
        "output_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uid</th>\n",
              "      <th>bot or not</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2214257545</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1847677113</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          uid  bot or not\n",
              "0  2214257545       False\n",
              "1  1847677113       False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}